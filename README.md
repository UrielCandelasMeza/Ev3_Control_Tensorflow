# Control de EV3 mediante Gestos de Manos ğŸ¤–ğŸ‘‹

## ğŸ“Œ DescripciÃ³n del Proyecto

Sistema que controla un robot LEGO EV3 mediante reconocimiento de gestos de mano (Forward, Back, Left, Right) usando **TensorFlow** y comunicaciÃ³n via WebSockets.

### Estructura del Proyecto

#### **Cliente (EV3)**

- **Hardware**: LEGO EV3 con `ev3dev2` como sistema operativo.
- **ConexiÃ³n**: SSH via Bluetooth (red PAN).
- **Archivos**:
  - `main.py`: Programa principal (gestiÃ³n de sockets y comandos).
  - `movement.py`: LÃ³gica de movimiento del EV3 (motores/sensores).

#### **Servidor (Procesamiento)**

- **Flujo**:
  1. Captura de imÃ¡genes (`collect.py`) con MediaPipe y OpenCV (300x300px).
  2. Entrenamiento del modelo (`train.py`) con TensorFlow (CNN).
  3. Servidor WebSocket (`main.py`) que envÃ­a predicciones al EV3.
- **Archivos auxiliares**:
  - `test.py`: Pruebas de conexiÃ³n.
  - `view.py`: Ejemplo de captura con OpenCV.

## ğŸ› ï¸ InstalaciÃ³n

### Requisitos para el Servidor

```bash
# LibrerÃ­as principales (Python 3.10+)
pip install tensorflow==2.18 opencv-python==4.11.0 mediapipe==0.10.21 numpy==1.26.4
```

### ConfiguraciÃ³n del EV3

1. **ConexiÃ³n Bluetooth**:
   - Emparejar EV3 con el computador.
   - Conectar via SSH:
     ```bash
     ssh robot@ev3dev
     ```
   - Ingresar contraseÃ±a: (`maker`)

## ğŸš€ Uso

### 1. Entrenar el Modelo

```bash
# Generar dataset (ejecutar antes de entrenar)
python collect.py
```

Para guardar las imagenes es necesario presionar la tecla "S"

```bash
# Entrenar modelo (guardado en Models/)
python train.py
```

Adicionalmente aÃ±ado el modelo que yo he hecho.

https://www.kaggle.com/models/candelasmezauriel/hand-gesture-detection

### 2. Iniciar Servidor

```bash
python main.py  # Inicia servidor WebSocket
```

### 3. Ejecutar Cliente en EV3

```bash
cd client
python3 main.py  # Desde la terminal del EV3
```

## ğŸ“‚ Estructura de Directorios

```
.
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ main.py          # Cliente principal
â”‚   â””â”€â”€ movement.py      # Control de motores
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ collect.py       # Captura de imÃ¡genes
â”‚   â”œâ”€â”€ train.py         # Entrenamiento
â”‚   â”œâ”€â”€ main.py          # Servidor WebSocket
â”‚   â”œâ”€â”€ test.py          # Pruebas
â”‚   â””â”€â”€ view.py          # Ejemplo OpenCV
â”œâ”€â”€ Datasets/            # ImÃ¡genes para entrenamiento
â”œâ”€â”€ Models/              # Modelos entrenados
â””â”€â”€ images/              # Capturas para documentaciÃ³n
```

## ğŸ–¼ï¸ Gestos Soportados

| Gestos  | Imagen de Referencia           | Comando EV3     |
| ------- | ------------------------------ | --------------- |
| Forward | ![Forward](images/Forward.jpg) | Avanzar         |
| Back    | ![Back](images/Back.jpg)       | Retroceder      |
| Left    | ![Left](images/Left.jpg)       | Girar izquierda |
| Right   | ![Right](images/Right.jpg)     | Girar derecha   |

## ğŸ“ Notas Adicionales

- El dataset debe contener al menos **200 imÃ¡genes por gesto** para un buen entrenamiento (En mi caso usÃ© **410**).
- Para mejor rendimiento, usa iluminaciÃ³n uniforme al capturar gestos y realizar distintas poses en diferentes fondos.
